\begin{thebibliography}{24}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aggarwal(2018{\natexlab{a}})]{inbook:Aggarwal-1.2}
Charu~C. Aggarwal.
\newblock \emph{Neural Networks and Deep Learning: A Textbook}, chapter 1.2.
\newblock Springer Publishing Company, Incorporated, 1st edition, 2018{\natexlab{a}}.
\newblock ISBN 3319944622.

\bibitem[Aggarwal(2018{\natexlab{b}})]{inbook:Aggarwal-3.2}
Charu~C. Aggarwal.
\newblock \emph{Neural Networks and Deep Learning: A Textbook}, chapter 3.2.
\newblock Springer Publishing Company, Incorporated, 1st edition, 2018{\natexlab{b}}.
\newblock ISBN 3319944622.

\bibitem[Aggarwal(2018{\natexlab{c}})]{inbook:Aggarwal-4.1}
Charu~C. Aggarwal.
\newblock \emph{Neural Networks and Deep Learning: A Textbook}, chapter 4.1.
\newblock Springer Publishing Company, Incorporated, 1st edition, 2018{\natexlab{c}}.
\newblock ISBN 3319944622.

\bibitem[AI-Wiki()]{WeightsBias}
AI-Wiki.
\newblock Weights and biases.
\newblock URL \url{https://machine-learning.paperspace.com/wiki/weights-and-biases}.
\newblock [Online; accessed April 30, 2024].

\bibitem[Andriushchenko et~al.(2023)Andriushchenko, D'Angelo, Varre, and Flammarion]{andriushchenko2023need}
Maksym Andriushchenko, Francesco D'Angelo, Aditya Varre, and Nicolas Flammarion.
\newblock Why do we need weight decay in modern deep learning?, 2023.

\bibitem[Brownlee(2020)]{capacity-Brownlee}
Jason Brownlee.
\newblock How to control neural network model capacity with nodes and layers, 2020.
\newblock URL \url{https://machinelearningmastery.com/how-to-control-neural-network-model-capacity-with-nodes-and-layers/}.
\newblock [Online; accessed April 30, 2024].

\bibitem[Brownlee(2021)]{SGD-BP}
Jason Brownlee.
\newblock Difference between backpropagation and stochastic gradient descent, 2021.
\newblock URL \url{https://machinelearningmastery.com/difference-between-backpropagation-and-stochastic-gradient-descent/}.
\newblock [Online; accessed May 14, 2024].

\bibitem[Brownlee(2022)]{batchvsEpoch}
Jason Brownlee.
\newblock Difference between a batch and an epoch in a neural network, 2022.
\newblock URL \url{https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/}.
\newblock [Online; accessed May 14, 2024].

\bibitem[Burnham and Anderson(2002)]{burnham2002model}
K.P. Burnham and D.R. Anderson.
\newblock \emph{Model selection and multimodel inference: a practical information-theoretic approach}.
\newblock Springer Verlag, 2002.

\bibitem[cdeterman()]{layers}
cdeterman.
\newblock what is a 'layer' in a neural network.
\newblock Stack Overflow.
\newblock URL \url{https://stackoverflow.com/questions/35345191/what-is-a-layer-in-a-neural-network/35347548#35347548}.

\bibitem[Ciampiconi et~al.(2023)Ciampiconi, Elwood, Leonardi, Mohamed, and Rozza]{ciampiconi2023survey}
Lorenzo Ciampiconi, Adam Elwood, Marco Leonardi, Ashraf Mohamed, and Alessandro Rozza.
\newblock A survey and taxonomy of loss functions in machine learning, 2023.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, and Courville]{Goodfellow-et-al-2016}
Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
\newblock \emph{Deep Learning}.
\newblock MIT Press, 2016.
\newblock \url{http://www.deeplearningbook.org}.

\bibitem[Hagan et~al.(2014)Hagan, Demuth, Beale, and Orlando]{Hagan_Martin}
Martin~T Hagan, Howard~B Demuth, Mark~H Beale, and Jes√∫s Orlando, De.
\newblock \emph{Neural Network Design (2nd Edition)}, chapter 4 Perceptron Learning Rule.
\newblock Martin Hagan, 2014.
\newblock ISBN 978-0971732117.
\newblock URL \url{https://hagan.okstate.edu/NNDesign.pdf}.

\bibitem[IBM(2024)]{web:IBM:NN}
IBM.
\newblock What are neural networks, 2024.
\newblock URL \url{https://www.ibm.com/topics/neural-networks}.
\newblock Accessed 7 Febuary 2024.

\bibitem[Jagtap and Karniadakis(2022)]{jagtap2022important}
Ameya~D. Jagtap and George~Em Karniadakis.
\newblock How important are activation functions in regression and classification? a survey, performance comparison, and future directions, 2022.

\bibitem[Kowalik(2023)]{capacity-Kowalik}
Marek Kowalik.
\newblock Capacities of quantum neural networks, part 1, 2023.
\newblock URL \url{https://medium.com/@marekkowalik97/capacities-of-quantum-neural-networks-part-1-1a731f44be0}.
\newblock [Online; accessed May 2, 2024].

\bibitem[Kuhn and Johnson(2013)]{Kuhn_13}
Max Kuhn and Kjell Johnson.
\newblock \emph{Applied Predictive Modeling}.
\newblock Springer, 2013.
\newblock ISBN 978-1-4614-6848-6.

\bibitem[Murphy(2012)]{MurphyML}
Kevin~P. Murphy.
\newblock \emph{Machine Learning: A Probabilistic Perspective}.
\newblock MIT Press, 2012.
\newblock URL \url{https://probml.github.io/pml-book/book0.html}.

\bibitem[Ruder(2017)]{ruder2017overview}
Sebastian Ruder.
\newblock An overview of gradient descent optimization algorithms, 2017.

\bibitem[Russel and Norvig(2010)]{book:AIModernApp}
Stuart Russel and Peter Norvig.
\newblock \emph{Artificial Intelligence, A Modern Approach}, chapter~18.
\newblock Pearson Education, New Jersey, 3rd edition, 2010.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov]{JMLR:v15:srivastava14a}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock \emph{Journal of Machine Learning Research}, 15\penalty0 (56):\penalty0 1929--1958, 2014.
\newblock URL \url{http://jmlr.org/papers/v15/srivastava14a.html}.

\bibitem[Vasani(2019)]{Vasani_2019}
Dipam Vasani.
\newblock This thing called weight decay, Nov 2019.
\newblock URL \url{https://towardsdatascience.com/this-thing-called-weight-decay-a7cd4bcfccab}.

\bibitem[Wakefield()]{GuideML}
Katrina Wakefield.
\newblock A guide to the types of machine learning algorithms and their applications.
\newblock URL \url{https://www.sas.com/en_gb/insights/articles/analytics/machine-learning-algorithms.html}.
\newblock [Online; accessed May 5, 2024].

\bibitem[{Wikipedia, the free encyclopedia}()]{overfitting-img}
{Wikipedia, the free encyclopedia}.
\newblock Overfitting.
\newblock URL \url{https://en.wikipedia.org/wiki/Overfitting}.
\newblock [Online; accessed April 30, 2024].

\end{thebibliography}
