\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\citation{GuideML}
\citation{web:IBM:NN}
\citation{inbook:Aggarwal-1.2}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Neural Networks}{3}{section.2}\protected@file@percent }
\citation{book:AIModernApp}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Single Layer Perceptron}{4}{subsection.2.1}\protected@file@percent }
\newlabel{sub:Single Layer Perceptron}{{2.1}{4}{Single Layer Perceptron}{subsection.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A single layer perceptron.}}{4}{figure.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Neurons}{4}{subsection.2.2}\protected@file@percent }
\newlabel{sub:Neurons}{{2.2}{4}{Neurons}{subsection.2.2}{}}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition\nobreakspace  {}2.1\else \numberline {2.1}Definition\fi \thmtformatoptarg {Neurons}}{4}{definition.2.1}\protected@file@percent }
\citation{WeightsBias}
\citation{book:AIModernApp}
\newlabel{Neuron}{{2.1}{5}{Neurons}{Item.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A neuron and its inputs and outputs.}}{5}{figure.2}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition\nobreakspace  {}2.2\else \numberline {2.2}Definition\fi \thmtformatoptarg {Weights}}{5}{definition.2.2}\protected@file@percent }
\newlabel{def:weight}{{2.2}{5}{Weights}{definition.2.2}{}}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition\nobreakspace  {}2.3\else \numberline {2.3}Definition\fi \thmtformatoptarg {Bias}}{5}{definition.2.3}\protected@file@percent }
\newlabel{def:bias}{{2.3}{5}{Bias}{definition.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Feed-Forward Neural Networks}{5}{subsection.2.3}\protected@file@percent }
\newlabel{sub:Feed-Forward Neural Networks}{{2.3}{5}{Feed-Forward Neural Networks}{subsection.2.3}{}}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition\nobreakspace  {}2.4\else \numberline {2.4}Definition\fi \thmtformatoptarg {Feed-forward Neural Network}}{5}{definition.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Layers}{5}{subsection.2.4}\protected@file@percent }
\newlabel{sub:Layers}{{2.4}{5}{Layers}{subsection.2.4}{}}
\citation{layers}
\citation{jagtap2022important}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A graph representation of a Neural Network.}}{6}{figure.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Activation Function}{6}{subsection.2.5}\protected@file@percent }
\newlabel{sub:ActivationFunction}{{2.5}{6}{Activation Function}{subsection.2.5}{}}
\@writefile{loe}{\contentsline {remark}{\ifthmt@listswap Remark\nobreakspace  {}2.5\else \numberline {2.5}Remark\fi \thmtformatoptarg {The vanishing and exploding gradient problems}}{6}{remark.2.5}\protected@file@percent }
\newlabel{actfun}{{2.5}{7}{Activation Function}{Item.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Common activation functions, their derivatives, range, and order of continuity.}}{7}{figure.4}\protected@file@percent }
\citation{inbook:Aggarwal-3.2}
\@writefile{toc}{\contentsline {section}{\numberline {3}Learning}{8}{section.3}\protected@file@percent }
\newlabel{sec:Learning}{{3}{8}{Learning}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}BackPropagation}{8}{subsection.3.1}\protected@file@percent }
\newlabel{sub:BackPropagation}{{3.1}{8}{BackPropagation}{subsection.3.1}{}}
\newlabel{simpleNet}{{3.1}{8}{BackPropagation}{subsection.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces A simple network with 2 inputs, one hidden layer, and two outputs.}}{8}{figure.5}\protected@file@percent }
\newlabel{simpleNet}{{3.1}{8}{BackPropagation}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Pre and Post activation values of a neuron.}}{8}{figure.6}\protected@file@percent }
\@writefile{loe}{\contentsline {theorem}{\ifthmt@listswap Theorem\nobreakspace  {}3.1\else \numberline {3.1}Theorem\fi \thmtformatoptarg {Multivariate Chain Rule}}{8}{theorem.3.1}\protected@file@percent }
\newlabel{chain}{{3.1}{8}{Multivariate Chain Rule}{theorem.3.1}{}}
\newlabel{deriv_o_j}{{1}{9}{BackPropagation}{equation.3.1}{}}
\citation{Hagan_Martin}
\citation{ruder2017overview}
\citation{ruder2017overview}
\newlabel{deriv_w}{{2}{10}{BackPropagation}{equation.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Weight Update Rule}{10}{subsection.3.2}\protected@file@percent }
\newlabel{sub:Weight Update Rule}{{3.2}{10}{Weight Update Rule}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Gradient Descent}{10}{subsection.3.3}\protected@file@percent }
\newlabel{sub:Gradient Descent}{{3.3}{10}{Gradient Descent}{subsection.3.3}{}}
\citation{ruder2017overview}
\citation{Goodfellow-et-al-2016}
\citation{SGD-BP}
\citation{ciampiconi2023survey}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Vanilla Gradient Descent}{11}{subsubsection.3.3.1}\protected@file@percent }
\newlabel{subsub:Vanilla Gradient Descent}{{3.3.1}{11}{Vanilla Gradient Descent}{subsubsection.3.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Stochastic Gradient Descent}{11}{subsubsection.3.3.2}\protected@file@percent }
\newlabel{subsub:Stochastic Gradient Descent}{{3.3.2}{11}{Stochastic Gradient Descent}{subsubsection.3.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Regression Loss Functions}{11}{subsection.3.4}\protected@file@percent }
\newlabel{sub:Loss Function}{{3.4}{11}{Regression Loss Functions}{subsection.3.4}{}}
\citation{ciampiconi2023survey}
\citation{ciampiconi2023survey}
\citation{ciampiconi2023survey}
\citation{ciampiconi2023survey}
\citation{ciampiconi2023survey}
\citation{ciampiconi2023survey}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Classification Loss Functions}{13}{subsection.3.5}\protected@file@percent }
\newlabel{sub:Classification Loss Functions}{{3.5}{13}{Classification Loss Functions}{subsection.3.5}{}}
\citation{ciampiconi2023survey}
\citation{Goodfellow-et-al-2016}
\citation{Goodfellow-et-al-2016}
\citation{Goodfellow-et-al-2016}
\citation{book:AIModernApp}
\citation{inbook:Aggarwal-4.1}
\citation{overfitting-img}
\citation{overfitting-img}
\citation{burnham2002model}
\@writefile{toc}{\contentsline {section}{\numberline {4}Fitting the model}{14}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Overfitting}{14}{subsection.4.1}\protected@file@percent }
\newlabel{sub:Overfitting}{{4.1}{14}{Overfitting}{subsection.4.1}{}}
\@writefile{loe}{\contentsline {example}{\ifthmt@listswap Example\nobreakspace  {}4.1\else \numberline {4.1}Example\fi }{14}{example.4.1}\protected@file@percent }
\citation{capacity-Brownlee}
\citation{capacity-Kowalik}
\citation{capacity-Kowalik}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Diagrams representing Underfitting, Appropriate, and Overfitting, from \citep  {Goodfellow-et-al-2016}}}{15}{figure.7}\protected@file@percent }
\@writefile{loe}{\contentsline {remark}{\ifthmt@listswap Remark\nobreakspace  {}4.2\else \numberline {4.2}Remark\fi }{15}{remark.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Underfitting}{15}{subsection.4.2}\protected@file@percent }
\newlabel{sub:Underfitting}{{4.2}{15}{Underfitting}{subsection.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Mastering the Fit}{15}{subsection.4.3}\protected@file@percent }
\newlabel{sub:Mastering the Fit}{{4.3}{15}{Mastering the Fit}{subsection.4.3}{}}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition\nobreakspace  {}4.3\else \numberline {4.3}Definition\fi \thmtformatoptarg {Capacity}}{15}{definition.4.3}\protected@file@percent }
\citation{Goodfellow-et-al-2016}
\citation{Goodfellow-et-al-2016}
\citation{MurphyML}
\citation{MurphyML}
\newlabel{Overfitting}{{4.1}{16}{Overfitting}{subsection.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces A diagram showing overfitting (green line) of data from \citep  {overfitting-img}}}{16}{figure.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Cross-Validation}{16}{subsection.4.4}\protected@file@percent }
\newlabel{sub:Cross-Validation}{{4.4}{16}{Cross-Validation}{subsection.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Regularization Techniques}{16}{subsection.4.5}\protected@file@percent }
\newlabel{sub:Regularization Techniques}{{4.5}{16}{Regularization Techniques}{subsection.4.5}{}}
\citation{Vasani_2019}
\citation{andriushchenko2023need}
\citation{andriushchenko2023need}
\citation{Kuhn_13}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces $k$-fold cross-validation}}{17}{algocf.1}\protected@file@percent }
\newlabel{k-CrossVal}{{1}{17}{Cross-Validation}{algocf.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.1}L2 Parameter Norm Penalty (Weight Decay)}{17}{subsubsection.4.5.1}\protected@file@percent }
\newlabel{sub:L1 and L2 Regularization}{{4.5.1}{17}{L2 Parameter Norm Penalty (Weight Decay)}{subsubsection.4.5.1}{}}
\newlabel{eq:general_loss}{{15}{17}{L2 Parameter Norm Penalty (Weight Decay)}{equation.4.15}{}}
\citation{Goodfellow-et-al-2016}
\citation{Goodfellow-et-al-2016}
\citation{Goodfellow-et-al-2016}
\citation{Goodfellow-et-al-2016}
\citation{JMLR:v15:srivastava14a}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Graph from \cite  {Goodfellow-et-al-2016}. The curves depict the evolution of the negative log-likelihood loss over the number of training iterations or epochs. Initially, the training objective steadily decreases over time as the model learns from the data. However, a notable observation is that the validation set average loss eventually starts to rise again after reaching a minimum, resulting in an asymmetric U-shaped curve. This pattern indicates that despite improvements in the training loss, the model's performance on unseen data begins to degrade over time, highlighting the phenomenon of overfitting.}}{18}{figure.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.2}Data Augmentation}{18}{subsubsection.4.5.2}\protected@file@percent }
\newlabel{sub:Data Augmentation}{{4.5.2}{18}{Data Augmentation}{subsubsection.4.5.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.3}Early Stopping}{18}{subsubsection.4.5.3}\protected@file@percent }
\newlabel{sub:Early Stopping}{{4.5.3}{18}{Early Stopping}{subsubsection.4.5.3}{}}
\bibdata{ML_Report_DRP2024}
\bibcite{inbook:Aggarwal-1.2}{{1}{2018{a}}{{Aggarwal}}{{}}}
\bibcite{inbook:Aggarwal-3.2}{{2}{2018{b}}{{Aggarwal}}{{}}}
\bibcite{inbook:Aggarwal-4.1}{{3}{2018{c}}{{Aggarwal}}{{}}}
\bibcite{WeightsBias}{{4}{}{{AI-Wiki}}{{}}}
\bibcite{andriushchenko2023need}{{5}{2023}{{Andriushchenko et~al.}}{{Andriushchenko, D'Angelo, Varre, and Flammarion}}}
\bibcite{capacity-Brownlee}{{6}{2020}{{Brownlee}}{{}}}
\bibcite{SGD-BP}{{7}{2021}{{Brownlee}}{{}}}
\bibcite{burnham2002model}{{8}{2002}{{Burnham and Anderson}}{{}}}
\bibcite{layers}{{9}{}{{cdeterman}}{{}}}
\bibcite{ciampiconi2023survey}{{10}{2023}{{Ciampiconi et~al.}}{{Ciampiconi, Elwood, Leonardi, Mohamed, and Rozza}}}
\bibcite{Goodfellow-et-al-2016}{{11}{2016}{{Goodfellow et~al.}}{{Goodfellow, Bengio, and Courville}}}
\bibcite{Hagan_Martin}{{12}{2014}{{Hagan et~al.}}{{Hagan, Demuth, Beale, and Orlando}}}
\bibcite{web:IBM:NN}{{13}{2024}{{IBM}}{{}}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.4}Dropout}{19}{subsubsection.4.5.4}\protected@file@percent }
\newlabel{sub:Dropout}{{4.5.4}{19}{Dropout}{subsubsection.4.5.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{19}{section.5}\protected@file@percent }
\newlabel{sec:Conclusion}{{5}{19}{Conclusion}{section.5}{}}
\bibcite{jagtap2022important}{{14}{2022}{{Jagtap and Karniadakis}}{{}}}
\bibcite{capacity-Kowalik}{{15}{2023}{{Kowalik}}{{}}}
\bibcite{Kuhn_13}{{16}{2013}{{Kuhn and Johnson}}{{}}}
\bibcite{MurphyML}{{17}{2012}{{Murphy}}{{}}}
\bibcite{ruder2017overview}{{18}{2017}{{Ruder}}{{}}}
\bibcite{book:AIModernApp}{{19}{2010}{{Russel and Norvig}}{{}}}
\bibcite{JMLR:v15:srivastava14a}{{20}{2014}{{Srivastava et~al.}}{{Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov}}}
\bibcite{Vasani_2019}{{21}{2019}{{Vasani}}{{}}}
\bibcite{GuideML}{{22}{}{{Wakefield}}{{}}}
\bibcite{overfitting-img}{{23}{}{{Wikipedia, the free encyclopedia}}{{}}}
\bibstyle{plainnat}
\newlabel{capacity}{{4.3}{20}{Mastering the Fit}{definition.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces A diagram showing the optimal capacity of a model and its relation with the \textit  {bias-variance trade-off}, from \citep  {capacity-Kowalik}}}{20}{figure.9}\protected@file@percent }
\gdef \@abspage@last{20}
