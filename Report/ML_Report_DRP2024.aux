\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\citation{GuideML}
\citation{SurveyML}
\citation{web:IBM:NN}
\citation{inbook:Aggarwal-1.2}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Neural Networks}{3}{section.2}\protected@file@percent }
\citation{inbook:Aggarwal-1.2}
\citation{inbook:Aggarwal-1.2}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Classification of Machine Learning Algorithms from \cite  {SurveyML}}}{4}{figure.1}\protected@file@percent }
\newlabel{fig:ML}{{1}{4}{Classification of Machine Learning Algorithms from \cite {SurveyML}}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Single Layer Perceptron}{4}{subsection.2.1}\protected@file@percent }
\newlabel{sub:Single Layer Perceptron}{{2.1}{4}{Single Layer Perceptron}{subsection.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A single layer perceptron, from \cite  {inbook:Aggarwal-1.2}.}}{4}{figure.2}\protected@file@percent }
\newlabel{fig:SLP}{{2}{4}{A single layer perceptron, from \cite {inbook:Aggarwal-1.2}}{figure.2}{}}
\citation{book:AIModernApp}
\citation{WeightsBias}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A neuron and its inputs and outputs.}}{5}{figure.3}\protected@file@percent }
\newlabel{fig:Neuron}{{3}{5}{A neuron and its inputs and outputs}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Neurons}{5}{subsection.2.2}\protected@file@percent }
\newlabel{sub:Neurons}{{2.2}{5}{Neurons}{subsection.2.2}{}}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition\nobreakspace  {}2.1\else \numberline {2.1}Definition\fi \thmtformatoptarg {Neurons}}{5}{definition.2.1}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition\nobreakspace  {}2.2\else \numberline {2.2}Definition\fi \thmtformatoptarg {Weights}}{5}{definition.2.2}\protected@file@percent }
\newlabel{def:weight}{{2.2}{5}{Weights}{definition.2.2}{}}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition\nobreakspace  {}2.3\else \numberline {2.3}Definition\fi \thmtformatoptarg {Bias}}{5}{definition.2.3}\protected@file@percent }
\newlabel{def:bias}{{2.3}{5}{Bias}{definition.2.3}{}}
\citation{book:AIModernApp}
\citation{inbook:Aggarwal-1.2}
\citation{inbook:Aggarwal-1.2}
\citation{layers}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Feed-Forward Neural Networks}{6}{subsection.2.3}\protected@file@percent }
\newlabel{sub:Feed-Forward Neural Networks}{{2.3}{6}{Feed-Forward Neural Networks}{subsection.2.3}{}}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition\nobreakspace  {}2.4\else \numberline {2.4}Definition\fi \thmtformatoptarg {Feed-forward Neural Network}}{6}{definition.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Layers}{6}{subsection.2.4}\protected@file@percent }
\newlabel{sub:Layers}{{2.4}{6}{Layers}{subsection.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces A graph representation of a Neural Network, from \cite  {inbook:Aggarwal-1.2}.}}{6}{figure.4}\protected@file@percent }
\newlabel{fig:NN}{{4}{6}{A graph representation of a Neural Network, from \cite {inbook:Aggarwal-1.2}}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Activation Function}{6}{subsection.2.5}\protected@file@percent }
\newlabel{sub:ActivationFunction}{{2.5}{6}{Activation Function}{subsection.2.5}{}}
\citation{jagtap2022important}
\citation{jagtap2022important}
\@writefile{loe}{\contentsline {remark}{\ifthmt@listswap Remark\nobreakspace  {}2.5\else \numberline {2.5}Remark\fi \thmtformatoptarg {The vanishing and exploding gradient problems}}{7}{remark.2.5}\protected@file@percent }
\citation{jagtap2022important}
\citation{jagtap2022important}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Common activation functions, their derivatives, range, and order of continuity, from \cite  {jagtap2022important}.}}{8}{figure.5}\protected@file@percent }
\newlabel{fig:actfun}{{5}{8}{Common activation functions, their derivatives, range, and order of continuity, from \cite {jagtap2022important}}{figure.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Learning}{8}{section.3}\protected@file@percent }
\newlabel{sec:Learning}{{3}{8}{Learning}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}BackPropagation}{8}{subsection.3.1}\protected@file@percent }
\newlabel{sub:BackPropagation}{{3.1}{8}{BackPropagation}{subsection.3.1}{}}
\citation{inbook:Aggarwal-3.2}
\@writefile{loe}{\contentsline {example}{\ifthmt@listswap Example\nobreakspace  {}3.1\else \numberline {3.1}Example\fi \thmtformatoptarg {}}{9}{example.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces A simple network with 2 inputs, one hidden layer, and two outputs.}}{9}{figure.6}\protected@file@percent }
\newlabel{fig:simpleNet}{{6}{9}{A simple network with 2 inputs, one hidden layer, and two outputs}{figure.6}{}}
\@writefile{loe}{\contentsline {theorem}{\ifthmt@listswap Theorem\nobreakspace  {}3.2\else \numberline {3.2}Theorem\fi \thmtformatoptarg {Multivariate Chain Rule}}{9}{theorem.3.2}\protected@file@percent }
\newlabel{chain}{{3.2}{9}{Multivariate Chain Rule}{theorem.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Pre and Post activation values of a neuron.}}{9}{figure.7}\protected@file@percent }
\newlabel{fig:ppNeuron}{{7}{9}{Pre and Post activation values of a neuron}{figure.7}{}}
\newlabel{deriv_o_j}{{1}{10}{BackPropagation}{equation.3.1}{}}
\newlabel{deriv_w}{{2}{10}{BackPropagation}{equation.3.2}{}}
\citation{Hagan_Martin}
\citation{ruder2017overview}
\citation{Goodfellow-et-al-2016}
\citation{batchvsEpoch}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Weight Update Rule}{11}{subsection.3.2}\protected@file@percent }
\newlabel{sub:Weight Update Rule}{{3.2}{11}{Weight Update Rule}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Gradient Descent}{11}{subsection.3.3}\protected@file@percent }
\newlabel{sub:Gradient Descent}{{3.3}{11}{Gradient Descent}{subsection.3.3}{}}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition\nobreakspace  {}3.3\else \numberline {3.3}Definition\fi \thmtformatoptarg {Sample}}{11}{definition.3.3}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition\nobreakspace  {}3.4\else \numberline {3.4}Definition\fi \thmtformatoptarg {Batch}}{11}{definition.3.4}\protected@file@percent }
\citation{ruder2017overview}
\citation{ruder2017overview}
\citation{ruder2017overview}
\citation{ruder2017overview}
\citation{ruder2017overview}
\citation{ruder2017overview}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition\nobreakspace  {}3.5\else \numberline {3.5}Definition\fi \thmtformatoptarg {Epoch}}{12}{definition.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Vanilla Gradient Descent}{12}{subsubsection.3.3.1}\protected@file@percent }
\newlabel{subsub:Vanilla Gradient Descent}{{3.3.1}{12}{Vanilla Gradient Descent}{subsubsection.3.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Stochastic Gradient Descent}{12}{subsubsection.3.3.2}\protected@file@percent }
\newlabel{subsub:Stochastic Gradient Descent}{{3.3.2}{12}{Stochastic Gradient Descent}{subsubsection.3.3.2}{}}
\citation{ruder2017overview}
\citation{ruder2017overview}
\citation{ciampiconi2023survey}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Mini-Batch Gradient Descent}{13}{subsubsection.3.3.3}\protected@file@percent }
\newlabel{sub:Mini-Batch Gradient Descent}{{3.3.3}{13}{Mini-Batch Gradient Descent}{subsubsection.3.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces SGD fluctuation, from \citep  {ruder2017overview}}}{13}{figure.8}\protected@file@percent }
\newlabel{fig:fluctuation-SGD}{{8}{13}{SGD fluctuation, from \citep {ruder2017overview}}{figure.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Loss Functions}{13}{subsection.3.4}\protected@file@percent }
\newlabel{sub:Loss Functions}{{3.4}{13}{Loss Functions}{subsection.3.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Regression Loss Functions}{13}{subsubsection.3.4.1}\protected@file@percent }
\newlabel{sub:Loss Function}{{3.4.1}{13}{Regression Loss Functions}{subsubsection.3.4.1}{}}
\citation{ciampiconi2023survey}
\citation{ciampiconi2023survey}
\citation{ciampiconi2023survey}
\citation{ciampiconi2023survey}
\citation{ciampiconi2023survey}
\citation{ciampiconi2023survey}
\citation{ciampiconi2023survey}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Classification Loss Functions}{15}{subsubsection.3.4.2}\protected@file@percent }
\newlabel{sub:Classification Loss Functions}{{3.4.2}{15}{Classification Loss Functions}{subsubsection.3.4.2}{}}
\citation{Goodfellow-et-al-2016}
\citation{Goodfellow-et-al-2016}
\citation{Goodfellow-et-al-2016}
\citation{book:AIModernApp}
\citation{inbook:Aggarwal-4.1}
\citation{overfitting-img}
\citation{overfitting-img}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Diagrams representing Underfitting, Appropriate, and Overfitting, from \citep  {Goodfellow-et-al-2016}}}{16}{figure.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Fitting the model}{16}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Overfitting}{16}{subsection.4.1}\protected@file@percent }
\newlabel{sub:Overfitting}{{4.1}{16}{Overfitting}{subsection.4.1}{}}
\citation{burnham2002model}
\citation{capacity-Brownlee}
\citation{capacity-Kowalik}
\citation{capacity-Kowalik}
\@writefile{loe}{\contentsline {example}{\ifthmt@listswap Example\nobreakspace  {}4.1\else \numberline {4.1}Example\fi }{17}{example.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces A diagram showing overfitting (green line) of data from \citep  {overfitting-img}}}{17}{figure.10}\protected@file@percent }
\newlabel{fig:Overfitting}{{10}{17}{A diagram showing overfitting (green line) of data from \citep {overfitting-img}}{figure.10}{}}
\@writefile{loe}{\contentsline {remark}{\ifthmt@listswap Remark\nobreakspace  {}4.2\else \numberline {4.2}Remark\fi }{17}{remark.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Underfitting}{17}{subsection.4.2}\protected@file@percent }
\newlabel{sub:Underfitting}{{4.2}{17}{Underfitting}{subsection.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Mastering the Fit}{17}{subsection.4.3}\protected@file@percent }
\newlabel{sub:Mastering the Fit}{{4.3}{17}{Mastering the Fit}{subsection.4.3}{}}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition\nobreakspace  {}4.3\else \numberline {4.3}Definition\fi \thmtformatoptarg {Capacity}}{17}{definition.4.3}\protected@file@percent }
\citation{Goodfellow-et-al-2016}
\citation{Goodfellow-et-al-2016}
\citation{MurphyML}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces A diagram showing the optimal capacity of a model and its relation with the \textit  {bias-variance trade-off}, from \citep  {capacity-Kowalik}.}}{18}{figure.11}\protected@file@percent }
\newlabel{fig:capacity}{{11}{18}{A diagram showing the optimal capacity of a model and its relation with the \textit {bias-variance trade-off}, from \citep {capacity-Kowalik}}{figure.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Cross-Validation}{18}{subsection.4.4}\protected@file@percent }
\newlabel{sub:Cross-Validation}{{4.4}{18}{Cross-Validation}{subsection.4.4}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces $k$-fold cross-validation}}{18}{algocf.1}\protected@file@percent }
\newlabel{k-CrossVal}{{1}{18}{Cross-Validation}{algocf.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Regularization Techniques}{18}{subsection.4.5}\protected@file@percent }
\newlabel{sub:Regularization Techniques}{{4.5}{18}{Regularization Techniques}{subsection.4.5}{}}
\citation{MurphyML}
\citation{Vasani_2019}
\citation{andriushchenko2023need}
\citation{andriushchenko2023need}
\citation{Kuhn_13}
\citation{Goodfellow-et-al-2016}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.1}L2 Parameter Norm Penalty (Weight Decay)}{19}{subsubsection.4.5.1}\protected@file@percent }
\newlabel{sub:L1 and L2 Regularization}{{4.5.1}{19}{L2 Parameter Norm Penalty (Weight Decay)}{subsubsection.4.5.1}{}}
\newlabel{eq:general_loss}{{15}{19}{L2 Parameter Norm Penalty (Weight Decay)}{equation.4.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.2}Data Augmentation}{19}{subsubsection.4.5.2}\protected@file@percent }
\newlabel{sub:Data Augmentation}{{4.5.2}{19}{Data Augmentation}{subsubsection.4.5.2}{}}
\citation{Goodfellow-et-al-2016}
\citation{Goodfellow-et-al-2016}
\citation{Goodfellow-et-al-2016}
\citation{JMLR:v15:srivastava14a}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Graph from \cite  {Goodfellow-et-al-2016}. The curves depict the evolution of the negative log-likelihood loss over the number of training iterations or epochs. Initially, the training objective steadily decreases over time as the model learns from the data, yet the validation set loss eventually starts to rise again after reaching a minimum. This pattern indicates that despite improvements in the training loss, the model's performance on unseen data begins to degrade over time, highlighting the phenomenon of overfitting.}}{20}{figure.12}\protected@file@percent }
\newlabel{fig:augmentData}{{12}{20}{Graph from \cite {Goodfellow-et-al-2016}. The curves depict the evolution of the negative log-likelihood loss over the number of training iterations or epochs. Initially, the training objective steadily decreases over time as the model learns from the data, yet the validation set loss eventually starts to rise again after reaching a minimum. This pattern indicates that despite improvements in the training loss, the model's performance on unseen data begins to degrade over time, highlighting the phenomenon of overfitting}{figure.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.3}Early Stopping}{20}{subsubsection.4.5.3}\protected@file@percent }
\newlabel{sub:Early Stopping}{{4.5.3}{20}{Early Stopping}{subsubsection.4.5.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.4}Dropout}{20}{subsubsection.4.5.4}\protected@file@percent }
\newlabel{sub:Dropout}{{4.5.4}{20}{Dropout}{subsubsection.4.5.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{20}{section.5}\protected@file@percent }
\newlabel{sec:Conclusion}{{5}{20}{Conclusion}{section.5}{}}
\bibdata{ML_Report_DRP2024}
\bibcite{inbook:Aggarwal-1.2}{{1}{2018{a}}{{Aggarwal}}{{}}}
\bibcite{inbook:Aggarwal-3.2}{{2}{2018{b}}{{Aggarwal}}{{}}}
\bibcite{inbook:Aggarwal-4.1}{{3}{2018{c}}{{Aggarwal}}{{}}}
\bibcite{WeightsBias}{{4}{}{{AI-Wiki}}{{}}}
\bibcite{andriushchenko2023need}{{5}{2023}{{Andriushchenko et~al.}}{{Andriushchenko, D'Angelo, Varre, and Flammarion}}}
\bibcite{capacity-Brownlee}{{6}{2020}{{Brownlee}}{{}}}
\bibcite{batchvsEpoch}{{7}{2022}{{Brownlee}}{{}}}
\bibcite{burnham2002model}{{8}{2002}{{Burnham and Anderson}}{{}}}
\bibcite{layers}{{9}{}{{cdeterman}}{{}}}
\bibcite{ciampiconi2023survey}{{10}{2023}{{Ciampiconi et~al.}}{{Ciampiconi, Elwood, Leonardi, Mohamed, and Rozza}}}
\bibcite{Goodfellow-et-al-2016}{{11}{2016}{{Goodfellow et~al.}}{{Goodfellow, Bengio, and Courville}}}
\bibcite{Hagan_Martin}{{12}{2014}{{Hagan et~al.}}{{Hagan, Demuth, Beale, and Orlando}}}
\bibcite{web:IBM:NN}{{13}{2024}{{IBM}}{{}}}
\bibcite{jagtap2022important}{{14}{2022}{{Jagtap and Karniadakis}}{{}}}
\bibcite{capacity-Kowalik}{{15}{2023}{{Kowalik}}{{}}}
\bibcite{Kuhn_13}{{16}{2013}{{Kuhn and Johnson}}{{}}}
\bibcite{MurphyML}{{17}{2012}{{Murphy}}{{}}}
\bibcite{SurveyML}{{18}{2020}{{N and Gupta}}{{}}}
\bibcite{ruder2017overview}{{19}{2017}{{Ruder}}{{}}}
\bibcite{book:AIModernApp}{{20}{2010}{{Russel and Norvig}}{{}}}
\bibcite{JMLR:v15:srivastava14a}{{21}{2014}{{Srivastava et~al.}}{{Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov}}}
\bibcite{Vasani_2019}{{22}{2019}{{Vasani}}{{}}}
\bibcite{GuideML}{{23}{}{{Wakefield}}{{}}}
\bibcite{overfitting-img}{{24}{}{{Wikipedia, the free encyclopedia}}{{}}}
\bibstyle{plainnat}
\gdef \@abspage@last{22}
